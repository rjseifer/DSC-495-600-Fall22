{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latent Semantic Analysis (LSA)](#Latent-Semantic-Analysis-(LSA))\n",
    "\n",
    "1. [What is Semantic Analysis?](#What-is-Semantic-Analysis?)\n",
    "2. [Latent Semantic Analysis (LSA)](#Latent-Semantic-Analysis-(LSA))\n",
    "3. [Comparing Terms Using the Left Singular Vectors in $T$](#Comparing-Terms-Using-the-Left-Singular-Vectors-in-$T$)   \n",
    "4. [Comparing Documents Using the Right Singular Vectors in $D$](#Comparing-Documents-Using-the-Right-Singular-Vectors-in-$D$)   \n",
    "5. [Examining the Singular Values,  $S$](#Examining-the-Singular-Values,-$S$).   \n",
    ")    \n",
    "6. [How to Use LSA](#How-to-Use-LSA)\n",
    "7. [LSA for Queries](LSA-for-Queries)\n",
    "\n",
    "\n",
    "\n",
    "# Latent Semantic Analysis (LSA)\n",
    "\n",
    "\n",
    "\n",
    "## What is Semantic Analysis?\n",
    "So far we've cleaned text data, learned how to format it into more useable forms and subsequently used those forms to calculate how similar two documents within a given corpus were.\n",
    "\n",
    "That's quite a bit for our semester!\n",
    "\n",
    "In this section of the program we move onto how we can use machines to quickly approximate the meaning of texts within a given corpus. In NLP this is known as semantic analysis. Merriam Webster defines semantics as the study of meanings. So in NLP we can think of this branch as us trying to figure out what a document is about, what are the document's \"topics\".\n",
    "\n",
    "We'll specifically look at latent semantic analysis (LSA) and it times permit.\n",
    "\n",
    "## Latent Semantic Analysis (LSA)\n",
    "\n",
    "Now that we've spent some time in linear algebra land let's return to NLP with latent semantic analysis. The idea behind this algorithm is to use SVD to uncover the semantics that are \"latent\" in the text. We'll be working from this 1990 paper published in the Journal of the American Society for Information Science by Deerwester et. al http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.1152&rep=rep1&type=pdf\n",
    "\n",
    "For this notebook we'll be working with this collection of sentences that were made up, hopefully they work!\n",
    "\n",
    "Let's go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Columbus happens to be the state capital of Ohio.\",\n",
    "            \"My girlfriend studies graphic design at Columbus State.\",\n",
    "            \"The Ohio State Buckeyes play football in Columbus, Ohio.\",\n",
    "            \"Columbus is a city built on bold ideas and fueled by relentless optimism.\",\n",
    "            \"Columbus is open and welcoming visitors and is dedicated to doing so safely.\",\n",
    "            \"Cats have tiny whiskers\",\n",
    "            \"My cat is named Brutus.\",\n",
    "            \"I have two cats, one is quite fluffy\",\n",
    "            \"The best coffee shop in Columbus is right by my house.\",\n",
    "            \"My one cat has brown fur, my other cat has no fur!\",\n",
    "            \"Kitty is another word for cat.\",\n",
    "            \"Have you seen how beautiful the sunrise can be in Columbus?\",\n",
    "            \"That stray cat is in the garbage can again.\",\n",
    "            \"Here kitty, kitty, kitty. Why do you play with me?\",\n",
    "            \"Hamsters, dogs, cats, and boas. All make great pets.\",\n",
    "            \"I cannot believe the city of Columbus makes you pay for parking.\",\n",
    "            \"I have two cats, and they are named Perry and Brie.\",\n",
    "            \"Cat adoptions have increased since the pandemic started.\",\n",
    "            \"Columbus has the best ice cream shops.\",\n",
    "            \"My mom has four dogs and four cats.\",\n",
    "            \"Columbus is a hockey town now.\",\n",
    "            \"My cat Bailey loves to eat cucumbers and frozen pumpkin.\",\n",
    "            \"I buy my cats lots of nice toys, but their favorite thing to play with are the twist ties from bread bags.\",\n",
    "            \"The roads are going to be bad in the Columbus neighborhood of German Village.\",\n",
    "            \"Columbus is one of four cities in Ohio.\",\n",
    "            \"My two cats like to fight a lot.\",\n",
    "            \"One of the best things about visiting Columbus is the chance to explore its fascinating and fun-filled neighborhoods.\",\n",
    "            \"Columbus is a great city for cat owners!\",\n",
    "            \"During the next season of Top Chef, Columbus native Avishar Barua will be a competitor.\",\n",
    "            \"I saw a woman walking her cat down the street!\"]\n",
    "\n",
    "basis_set = [\"columbus\", \"state\", \"ohio\", \"city\", \n",
    "             \"play\", \"cats\", \"cat\", \"kitty\", \"fur\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you go through these 30 sentences you may sense a few of topics. LSA will work to detect these.\n",
    "\n",
    "Let's first start by taking these silly sentences, cleaning them up, and making a very specific document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term = np.zeros((len(sentences),len(basis_set)))\n",
    "\n",
    "word_dict = {}\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    word_dict[i] = {}\n",
    "    for word in [word.lower() for word in tokenizer.tokenize(sentences[i])]:\n",
    "        if word in basis_set:\n",
    "            if word in word_dict[i].keys():\n",
    "                word_dict[i][word] = word_dict[i][word] + 1\n",
    "            else:\n",
    "                word_dict[i][word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in word_dict.keys():\n",
    "    for j in range(len(basis_set)):\n",
    "        if basis_set[j] in word_dict[i].keys():\n",
    "            doc_term[i,j] = word_dict[i][basis_set[j]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_term, \n",
    "             columns = basis_set, \n",
    "             index = ['doc_' + str(i) for i in range(len(sentences))]).sort_values(['columbus',\n",
    "                                                                                    'state',\n",
    "                                                                                    'ohio',\n",
    "                                                                                    'city',\n",
    "                                                                                    'play',\n",
    "                                                                                    'cats',\n",
    "                                                                                    'cat',\n",
    "                                                                                    'kitty',\n",
    "                                                                                    'fur'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we've generated above is a word count vector using a small basis set of the most frequently used word. (Note this is only so we can clearly convey what is going on with LSA using 2-dimensions, which is usually not possible with real corpuses).\n",
    "\n",
    "__Now we need the transpose of the document-term frequency matrix, this is known as the term-document matrix__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc = doc_term.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(term_doc, \n",
    "             index = basis_set, \n",
    "             columns = ['doc_' + str(i) for i in range(len(sentences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA is essentially the utilization of the SVD of the term-document matrix.\n",
    "\n",
    "### You Code\n",
    "First find the SVD of `term_doc` below and then we'll go through how we can use it. In reasons that will become obvious soon call the $*$ matrix `T`, call the $\\sum$ matrix `S`, and call the $V^T$ matrix `Dt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that np.linalg.svd returns U S and V^T\n",
    "T,S,Dt = np.linalg.svd(term_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LSA to work we replace the term-document matrix with the approximation of the matrix that corresponds taking the first  singular directions, where  is the rank of the term-document matrix. Run the below code to make this dimensionality adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## np.linalg.matrix_rank gives the rank of the matrix\n",
    "rank = np.linalg.matrix_rank(term_doc)\n",
    "\n",
    "# We'll now replace all of our SVD output with \n",
    "# our lower rank approximation version.\n",
    "T = T[:,:rank]\n",
    "Dt = Dt[:rank,:]\n",
    "D = Dt.transpose()\n",
    "Sigma = np.zeros((rank,rank))\n",
    "\n",
    "for i in range(len(S)):\n",
    "    Sigma[i,i] = S[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The term-document matrix is a\",np.shape(term_doc),\"matrix\")\n",
    "print(\"====================================\")\n",
    "print(\"T is a\",np.shape(T),\"matrix\")\n",
    "print(\"Sigma is a\",np.shape(Sigma),\"matrix\")\n",
    "print(\"D transpose is a\",np.shape(Dt),\"matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Terms Using the Left Singular Vectors in $T$ \n",
    "Let $A$ denote our term-document matrix. Then each row of $A$ represents a measure of how often one of our terms appears in a given document. Taking the dot product of two separate rows of $A$ reflects how often two terms tend to occur together across our documents. We can do this simultaneously for all pairs of documents by computing:\n",
    "\n",
    "$$\n",
    "AA^T = T \\Sigma^{2}T^T,\n",
    "$$\n",
    "\n",
    "and thus the $i,j$  cell of $AA^T$  is found by taking the dot product of the $i$  and $j$  rows of $T\\Sigma$.\n",
    "\n",
    "In particular, we can think of the columns of $T\\Sigma$ as provide coordinates for our terms, i.e. providing vectors for the terms that more accurately reflect which terms occur together.\n",
    "\n",
    "Compute `TS` below and plot the first two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "# Hint use plt.scatter\n",
    "# Begin Plot\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "# Leave this code in to see what terms are represented by what points\n",
    "for i in range(len(basis_set)):\n",
    "    plt.text(T.dot(Sigma)[i,0], T.dot(Sigma)[i,1], basis_set[i], fontsize=14)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you come up with a \"topic\" or \"meaning\" that is captured by the first singular direction? What about the second?\n",
    "\n",
    "This plot worked well for us here because we had a toy example, another approach to investigating $T\\Sigma$ is to make a dataframe where each column is a column of $T\\Sigma$ and your index are the terms represented in each row.\n",
    "\n",
    "Do that below, then sort according to the first and second singular directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "term_space = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Documents Using the Right Singular Vectors in $D$\n",
    "This process will be similar.\n",
    "\n",
    "Again consider our term-document matrix $A$. columns of $A$ represent the profile of terms from our basis set used in each document. In this setting the dot product between two distinct columns of $A$ will tell us how similar two documents are with regard to their term usage. To compute the dot product between all unique documents all at once we find:\n",
    "\n",
    "$$\n",
    "AA^T = D \\Sigma^{2}D^T,\n",
    "$$\n",
    "\n",
    "and once again we note that the $i,j$ entry of $AA^T$ is precisely the dot product between the $i$ and $j$ rows of $D \\Sigma$.\n",
    "\n",
    "Just in the same way as we examined the \"term space\" above we can examine the \"document space\", however, we are now more equipped explorers because we have our knowledge from when we explored the term space, i.e. we know that the first singular direction (first column of $D \\Sigma$) measures \"Columbusness\" and the second singular direction (second column of $D \\Sigma$) measures the \"Catness\" of each document.\n",
    "\n",
    "Compute `DS` below and plot the first two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "# Hint use plt.scatter\n",
    "# Begin Figure\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "# Leave this code in to see what sentences are represented by what points\n",
    "for i in range(len(sentences)):\n",
    "    plt.text(D.dot(Sigma)[i,0], D.dot(Sigma)[i,1], sentences[i], fontsize=14)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Singular Values,  $S$\n",
    "\n",
    "So we've looked at the \"term-space\" the \"document-space\", but we have yet to look at the singular values themselves. We'll use these much like you would use the explained variance ratio from PCA. Recall the Eckart-Young theorem:\n",
    "\n",
    "$$\n",
    "\\min_{rank(B)=k} ||A-B||_2 = ||A -A_k||_2 = \\sigma_{k+1}\n",
    "$$\n",
    "\n",
    " \n",
    "where $A_k$ is the rank $k$  approximation of the original matrix $A$.\n",
    "\n",
    "So we can use singular values $2$ through $9$ to calculate the reconstruction accuracy (take one minus the error and divide by the number of entries in $A$). This can help us determine how many singular directions we should look at when reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(1,len(S)):\n",
    "    accs.append(1-S[i]/np.product(np.shape(term_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(1,len(accs)+1),accs, '-o')\n",
    "\n",
    "plt.xlabel(\"Number of Singular Value Dimensions\", fontsize=16)\n",
    "plt.ylabel(\"Reconstruction Accuracy\", fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use LSA\n",
    "LSA can be used in a number of different settings including both supervised (think classification) and unsupervised (think clustering) learning.\n",
    "\n",
    "For an example of LSA in classification let's harken back to our time tested example SMS spam classification. You could project documents into a lower dimensional \"document space\" and then use this lower dimensional data set as the input for the classification algorithm of your choice.\n",
    "\n",
    "To use LSA in clustering, you could again project either the terms or documents into the lower dimensional spaces (depending on your desired use case) and then cluster them with another clustering algorithm. As an example, maybe you're interested in the differences in language between sci-fi novels and historical fictions, or maybe the lyrical differences between two genres of music.\n",
    "\n",
    "To finish this notebook we'll look at the original intent of LSA, document retrieval. This task is why this technique was originally known as latent semantic indexing.\n",
    "\n",
    "## LSA for Queries\n",
    "Suppose somebody wants to find all documents in our corpus that are associated with \"kitty play\". One way to do this would be to just search for an exact match to \"kitty play\", however, maybe there are documents that say something like \"My cat likes to play outside\". This should still be associated with the phrase \"kitty play\", but not \"Buckeyes play\" right?\n",
    "\n",
    "This one way we can leverage LSA. LSA groups words that occur together, and we can remember in our corpus that \"kitty\" and \"cat\" are associated.\n",
    "\n",
    "### Step 1. Make a Term-Document Row for the Pseudo-Document\n",
    "Look back at our term-document matrix above and make the row that would result if we had included \"kitty play\" in our original corpus. Call it `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis set was:\n",
    "# columbus \tstate \tohio \tcity \tplay \tcats \tcat \tkitty \tfur\n",
    "q = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Make \n",
    "We need to derive a representation for our query, `q` in the document space. We'll call this $D_q$. It can be shown that the appropriate formula for $D_q$ is:\n",
    "\n",
    "$$\n",
    "D_q = qT\\Sigma^{-1}\n",
    "$$\n",
    "\n",
    "Using the fact that `np.linalg.inv` computes the inverse of a matrix find `Dq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dq = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot  $D_q$  as a large red dot in the 2 dimensional document space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint you can control the color of a dot in plt.scatter with c = \n",
    "# you can control the size of a dot in plt.scatter with s =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Finding Semantically Similar Documents\n",
    "You can now use the semantic vector versions of the documents to find vectors that are similar by calculating the cosine similarities to your query in the document space. Similar documents are ones whose similarity is larger than some threshold you can set.\n",
    "\n",
    "Find all documents in our set whose semantic vector representations have a cosine similarity with $D_q$  of at least $.9$ in the document space. Use the 2-D representation of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(u):\n",
    "    return np.sqrt(np.sum(np.power(u,2)))\n",
    "\n",
    "## You write a function to get the cosine similarity\n",
    "def cos_sim(u,v):\n",
    "    if norm(u)*norm(v) > 0:\n",
    "        return (u.dot(v))/(norm(u)*norm(v))\n",
    "    else:\n",
    "        return np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Vectors and TF-IDF\n",
    "As a final note, while we performed LSA on the word count vectors, you can easily go through the same process for frequency or tf-idf vectors. What works best just depends on your data and desired use case. For instance, maybe your corpus has texts of greatly varying lengths. In that case it would be best to scale your data for some reason. Note it may also be good practice to normalize your vectors by their $2$-norm prior to the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
