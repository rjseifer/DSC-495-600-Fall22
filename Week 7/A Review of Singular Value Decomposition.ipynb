{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Review of Singular Value Decomposition](#A-Review-of-Singular-Value-Decomposition)\n",
    "\n",
    "1. [SVD](#SVD)  \n",
    "2. [Performing SVD Using numpy](#Performing-SVD-Using-numpy)     \n",
    "3. [SVD's Relationship to $A^TA$](#SVD's-Relationship-to-$A^TA$)   \n",
    "4. [Dimensionality Reduction](#Dimensionality-Reduction)   \n",
    "5. [Dimensionality Reduction - Reducing the Number of Columns](#Dimensionality-Reduction---Reducing-the-Number-Columns)   \n",
    "\n",
    "\n",
    "# A Review of Singular Value Decomposition\n",
    "\n",
    "Before moving on to Latent Semantic Analysis I want to take a moment to review the workhorse behind that algorithm, Singular Value Decompositions (SVD). The SVD is a way to decompose a matrix that is incredibly valuable in data science and machine learning. Even if you've haven't heard of the SVD before, you've almost certainly used it. Anytime you use `sklearn`'s `PCA` you're running your data matrix through an SVD decomposition! Let's get going and dive into SVD.\n",
    "\n",
    "## SVD\n",
    "Suppose you have an $m\\times n$ real matrix, $A$. Then it is a fact that:\n",
    "\n",
    "$$\n",
    "A = U\\Sigma^TV^T\n",
    "$$\n",
    "\n",
    "where $U$ is an $m \\times m$ orthogonal real matrix ($UU^T = I$), $V$ is an $n \\times n$ orthogonal real matrix and $\\Sigma$ is an $m\\times n$ diagonal matrix of the same rank as $A$. The entries of $\\Sigma$ are known as the singular values of $A$ and are denoted as $\\sigma_i$ for $i = 1, \\dots,p$, $p= \\min{m,n}$. It is standard to let $\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq \\sigma_p$. The singular values of $A$ are the square roots of the nonzero eigenvalues of $A^TA$ and $AA^T$.\n",
    "\n",
    "The columns of $U$ are known as the left singular vectors of $A$ and the columns of $V$ are known as the right singular vectors of $A$.\n",
    "\n",
    "## Performing SVD Using numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random matrix\n",
    "A = np.random.randn(100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.svd returns the svd decomposition of a matrix\n",
    "# Note that linalg.svd returns V transpose not V itself\n",
    "U,S,Vt = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Vt.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD's Relationship to $A^TA$\n",
    "SVD has a nice relationship with $A^TA$.\n",
    "\n",
    "If the SVD of $A$ is $U\\Sigma^TV^T$ then:\n",
    "\n",
    "$$\n",
    "A^TA = (V\\Sigma^TU^T)(U\\Sigma V^T)= V\\Sigma^2V^T\n",
    "$$\n",
    "\n",
    "\n",
    "because $U$ is an orthogonal matrix, and thus $\\Sigma^2$ holds the eigenvalues of $A^TA$ and $V$ holds the corresponding eigenvectors.\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "AA^T = U\\Sigma^2U^T\n",
    "$$\n",
    "\n",
    "and thus $\\Sigma^2$ also holds the eigenvalues of $AA^T$and $U$ holds the corresponding eigenvectors.\n",
    "\n",
    "### Note on PCA\n",
    "If $A$ is a data matrix with centered columns then the above fact shows SVD's relationship to principal components analysis (PCA).\n",
    "\n",
    "## Dimensionality Reduction\n",
    "SVD is incredibly useful for dimensionality reduction. When I was preparing for this notebook I easily got confused because there are a couple different things people mean by dimensionality reduction when it comes to SVD. Let's first touch on the data compression interpretation.\n",
    "\n",
    "### Data Compression\n",
    "Consider the following theorem due to Eckart and Young.\n",
    "\n",
    "Suppose the SVD of $A$ is as described above. If $k<r = rank(A)$ and $A_k= \\sum_{i=1}^{k}\\sigma_iu_iv_i^T$, then\n",
    "\n",
    "$$\n",
    "\\min_{rank(B)=k}||A-B||_2 = |||A-A_k||_2 = \\sigma{k+1}\n",
    "$$\n",
    "\n",
    "\n",
    "_Fun fact: You can use this to compute the explained variance ratio for PCA_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's \"test\" this out with our matrix decomposition!\n",
    "test = S[0]*U[:,0].reshape(-1,1).dot(V[:,0].reshape(-1,1).transpose())\n",
    "\n",
    "print(np.linalg.norm(test - A, 2))\n",
    "print(S[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is where I got confused. When I think of dimensionality reduction, I don't picture this. If $A$ is a data matrix, $A_k$ has the same dimensionality as $A$  (same number of columns). However, it is data reduction in the sense that I can compress the amount of data I need to store for a decent approximation of $A$ by using a lower rank matrix. This approach is used in a lot of fields. One example is image compression http://www.math.utah.edu/~goller/F15_M2270/BradyMathews_SVDImage.pdf.\n",
    "\n",
    "## Dimensionality Reduction - Reducing the Number of Columns\n",
    "Let's see how we can perform dimensionality reduction in the way that we usually think about it in data science by considering the sample matrix we've been using in this notebook.\n",
    "\n",
    "Now suppose we want to reduce our example $A$ from a $100 \\times 4$  to a $100 \\times 2$ matrix. We can do this by multiplying the first $2$ columns of $U$ with the upper left $2 \\times 2$ block of $\\Sigma$, i.e. we reduce the dimensions of $A$ by calculating $U_2\\times V_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = np.zeros(np.shape(A))\n",
    "for i in range(len(S)):\n",
    "    Sigma[i,i] = S[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[:,:2].dot(Sigma[:2,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this in general by keeping the first $k$ columns of $U$ and the upper right $k\\times k$ block of $\\Sigma$, for $k \\leq rank{A}$.\n",
    "\n",
    "### Alternatively\n",
    "Another way to reduce $A$ to a lower dimensional matrix using SVD is to use `sklearn`'s `TruncatedSVD` object. While we can do this by hand using `numpy`, `TruncatedSVD` is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_svd = TruncatedSVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_svd.fit_transform(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign, Sign Everywhere a Sign\n",
    "You may notice that the signs of the two computations are opposite. That happens sometimes as the sign is determined by a random process in the code behind the scenes. This is fine in principle since the vectors will be the same but flipped.\n",
    "\n",
    "### But Why?\n",
    "So why are we doing this? That's a good question. I've tried to search around and figure out why we take this approach for dimensionality reduction with SVD since we're seemingly leaving the  matrix out to dry. This is the best explanation I can come up with.\n",
    "\n",
    "It holds that:\n",
    "\n",
    "$$\n",
    "AV = U\\Sigma.\n",
    "$$\n",
    "\n",
    "For the reduction of  to  dimensions:\n",
    "\n",
    "$$\n",
    "AV_k = U_k\\Sigma_k.\n",
    "$$\n",
    "\n",
    "The left hand side results in a matrix of the scalar projections of each observation from $A$ onto each column of $V_k$ (i.e. going from $n$ columns for each observation to $k$ columns for each observation). In the case of a scaled and cenetered data matrix, $A$, this is precisely the principal component values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check this like so\n",
    "svd_test = trunc_svd.fit_transform(A - np.mean(A,axis=0)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca_test = pca.fit_transform(A)[:,:2].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test == svd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a continuation of why we're looking at $U\\Sigma$ for projecting into a lower dimensional space, it has to do with the typical organization of a data matrix. Typically, the rows of a data matrix correspond to individual observations, while the columns represent independent variables.\n",
    "\n",
    "The right singular vectors (columns of $V$) represent the orthogonal directions in the original dataspace that best represent the original data in that lower dimension, i.e. are closest to $A$ in $2$-norm.\n",
    "\n",
    "So when we project onto the first $k$ columns of $V$ (compute $U_k\\Sigma_k$, we're projecting into the $k$-dimensional space that most closely approximates $A$ in $2$-norm.\n",
    "\n",
    "### Ahh, How Refreshing\n",
    "Now that we've refreshed ourselves about SVD let's continue to semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
