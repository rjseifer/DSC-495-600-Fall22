{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word2Vec](#Word2Vec)\n",
    "\n",
    "1. [Introduction](#Introduction)   \n",
    "2. [References](#References)   \n",
    "3. [The Idea Behind the Approach - Using Context Clues](#The-Idea-Behind-the-Approach---Using-Context-Clues)     \n",
    "4. [Searching for Context Clues: Skip-Grams](#Searching-for-Context-Clues:-Skip-Grams)   \n",
    "5. [The Neural Network](#The-Neural-Network)   \n",
    "6. [Training Word2Vec is Costly](#Training-Word2Vec-is-Costly)   \n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "## Introduction\n",
    "In this section we'll be diving into how neural networks can be used to represent NLP data in what are known as embeddings. They are so called because we are taking higher dimensional data and finding clever ways to embed it into a smaller dimensional space.\n",
    "\n",
    "By clever I mean that we'll be finding ways to embed that preserve some inherent nature of the data, for instance in a way that makes sense given a word's meaning in a piece of text. For example, a good word embedding should probably group the words \"apple\" and \"pears\" together (depending on the data set of course).\n",
    "\n",
    "\n",
    "The first feed forward neural network based word embedding we'll touch on is Word2Vec. Word2Vec was an approach developed by Mikolov et. al. across two papers in 2013 (https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546).\n",
    "\n",
    "The key idea behind this approach is to look at the words that occur next to the word you're interested in across your corpus, an idea known as looking at the context surround the target word.\n",
    "\n",
    "In this notebook we'll learn how to implement Word2Vec using an algorithm known as skip-gram with negative sampling (SGNS).\n",
    "\n",
    "## References\n",
    "\n",
    "Notably, Mikolov et. al.'s papers are a bit hard to decipher, so we'll actually be working off of a collection of other sources that are much more reader-friendly (i.e. I can understand them).\n",
    "\n",
    "This notebook will build heavily off of the following sources:\n",
    "\n",
    "* https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "* https://www.tensorflow.org/tutorials/text/word2vec\n",
    "* https://arxiv.org/abs/1411.2738\n",
    "* https://www.youtube.com/watch?v=D-ekE-Wlcds\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll import these, but I don't know how much \n",
    "## we'll use them\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind the Approach - Using Context Clues\n",
    "Before diving into the nitty-gritty lets dive into the idea behind the approach.\n",
    "\n",
    "\n",
    "\n",
    " You shall know a word by the company it keeps \n",
    "\n",
    "J.R. Firth\n",
    "\n",
    "\n",
    "At the heart of Word2Vec is the desire to learn the \"meaning\" of a word using context clues. Let's look at a simple example. Imagine you've picked up the hottest young adult novel set in a far off dystopian future. As many such novels do the author has created a \"fun\" future slang for their dystopian future and, being from the present, the words are unfamiliar to you. You come upon the following sentence, \"After work he needed to deposit his weekly pay at the kakoonahole.\"\n",
    "\n",
    "I would be shocked if you have heard the word kakoonahole before, but you probably have a rough idea that the author is using this word to represent a banking establishment of some kind.\n",
    "\n",
    "What gave that away? Neighboring words like \"weekly pay\" and \"deposit\".\n",
    "\n",
    "This is precisely the idea behind Word2Vec.\n",
    "\n",
    "With Word2Vec we will see how we can use a very simple feed forward neural network architecture (a single hidden layer) to produce representations of words as short dense vectors, as opposed to the standard long sparse vectors we've used up to this point (one-hot encodings, frequency vectors, tf-idf vectors).\n",
    "\n",
    "Importantly, these short dense vectors have been demonstrated to provide intuitive results that outperform previous techniques (like LSA) at certain tasks.\n",
    "\n",
    "## Searching for Context Clues: Skip-Grams\n",
    "When we use the phrase \"context clues\", we mean the words surrounding the word in which we have an interest. We can quantify the context words surrounding our target word using skip-grams.\n",
    "\n",
    "Consider this sample sentence.\n",
    "\n",
    "My cat sits in the sun\n",
    "\n",
    "For skip-grams we focus on a single word, and the window around that word. In this example let's choose a window size of 2. To create a collection of skip-grams you choose a target word, let's say it is sits, and look at all the words within the window size on either side. The skip-grams are then the collection of target word-window word pairings. For sits and size 2 this gives (sits, my), (sits, cat), (sits, in), (sits, the).\n",
    "\n",
    "Before moving on let's do a short practice to register understanding. Write down the skip-grams for the word cat.\n",
    "\n",
    "ANSWER\n",
    "(cat, My), (cat, sits), (cat, in).\n",
    "\n",
    "### Making skip-grams in keras\n",
    "We can use keras to quickly make skip-grams for us, let's see how!\n",
    "\n",
    "Preparing the data\n",
    "First you'll need to turn your text into a list of indices, like the imdb data set from the neural networks folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we'll tokenize our data using simple string functions\n",
    "sample_sentence = \"My cat sits in the sun\"\n",
    "\n",
    "tokens = list(sample_sentence.lower().split())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a word index dictionary\n",
    "word_index = {}\n",
    "i = 1\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in word_index.keys():\n",
    "        word_index[word] = i\n",
    "        i = i + 1\n",
    "        \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a reverse index as well\n",
    "reverse_word_index = {i: word for word,i in word_index.items()}\n",
    "\n",
    "print(reverse_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now create a sequence for our sentence like so\n",
    "sample_sequence = [word_index[word] for word in tokens]\n",
    "print(sample_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the skip-grams\n",
    "Now that we have a sequence for our sentence we can use `keras` to create a set of skip-grams for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large you want your windows to be\n",
    "window_size = 2\n",
    "\n",
    "# how many words are in your vocabulary?\n",
    "vocabulary_size = len(word_index.keys())\n",
    "\n",
    "# ignore the negative_samples argument for now\n",
    "# more on that later\n",
    "positive_skip_grams, _ = sequence.skipgrams(sample_sequence, \n",
    "                                  vocabulary_size=vocabulary_size,\n",
    "                                  window_size=window_size,\n",
    "                                  negative_samples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sentence was:\")\n",
    "print(sample_sentence)\n",
    "\n",
    "print(\"######################\")\n",
    "\n",
    "print(\"The skip-grams are:\")\n",
    "for item in positive_skip_grams:\n",
    "    print(reverse_word_index[item[0]],reverse_word_index[item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "Use the next few code chunks to read in the imdb data set from `keras`. The practice by calculating the skip-grams for the a couple of reviews from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "(imdb_train,y_train_imdb), (imdb_test,y_test_imdb) = imdb.load_data(num_words=n, \n",
    "                                                                                seed=440,\n",
    "                                                                                index_from=3)\n",
    "\n",
    "# word_index is a dictionary that maps each word to it's index\n",
    "imdb_word_index = imdb.get_word_index()\n",
    "\n",
    "# We now adjust the indices according to the coding presented above\n",
    "imdb_word_index = {key:(value+3) for key,value in imdb_word_index.items()}\n",
    "\n",
    "imdb_word_index[\"<PAD>\"] = 0\n",
    "imdb_word_index[\"<START>\"] = 1\n",
    "imdb_word_index[\"<UNKNOWN WORD>\"] = 2\n",
    "imdb_word_index[\"<UNUSED WORD>\"] = 3\n",
    "\n",
    "imdb_reverse_index = dict([(value,key) for (key,value) in imdb_word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You Code Here\n",
    "### Choose a few reviews and find their skip-grams here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "The neural network used for __Word2Vec__, as previously mentioned, has a very simple architecture and we will introduce it now.\n",
    "\n",
    "Suppose you wish to represent a vocabulary of $M$ unique words/tokens from your corpus. You then build a network with `M` input nodes, one corresponding to each word in the vocabulary, a single hidden layer with `N` nodes where , and an output layer with M nodes, again corresponding to each word in the vocabulary.\n",
    "\n",
    "Its architecture looks like this \n",
    "\n",
    "As you can see from the architecture, the activation from the input layer to the hidden layer is the identity function and the activation from the hidden layer to the output layer is the softmax function, which is defined for a vector $z$ with $K$ entries as:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum^{K}_{j=1}e^{z_j}},\n",
    "$$\n",
    " \n",
    "and thus transforms the output from nodes of real numbers to a probability distribution.\n",
    "\n",
    "### Why The Softmax?\n",
    "The genius of the __Word2Vec__ approach was that it turned the semantics problem, \"what does word $w$ mean?\" into a context problem, \"what is the context of $w$?\" is then turned into a multiclass classification problem.\n",
    "\n",
    "With the above neural net the goal of the network is to model the probability that word \n",
    " is a contextual word (in the skip-gram window) of a given target word \n",
    ", i.e. we're modeling the conditional probability:\n",
    "\n",
    "$$\n",
    "p(w_j|w_t)\\sim \\sigma(O)_j= e^{O_j}/\\sum^{K}_{i=1}e^{O_i}\n",
    "$$\n",
    "\n",
    "where I'm lazily letting $O_i$ denote the output of the weighted sum of the hidden layer nodes at output node $i$.\n",
    "\n",
    "### What is the Training Data?\n",
    "The training data for this network is produced from the skipgrams. An $X,y$  pair in the training set would be as follows. $X$ would be a one hot encoded vector where all $M$ entries are $0$  except for the entry corresponding to the target word of interest. $y$ is a one hot encoded vector where all $M$ entries are $0$ except for the entry corresponding to the context word.\n",
    "\n",
    "### Weights are Where it's at\n",
    "However, we don't care at all about using the network to make predictions, we just want the weight matrices that result.\n",
    "\n",
    "Let $W$ be the $N\\times M$  trained weight matrix for the input layer into the hidden layer, and let $x_i(M\\times1)$ be a one-hot encoded vector corresponding to word $w_i$, then the word2vec embedding of $w_i$ is simply $Ww_i$ which is the $i$^{th} column of $W$ .\n",
    "\n",
    "You Code, A Very Simple Example\n",
    "I've coded up the skip grams for a very simple example below and created the $X$ and $y$ for you. Using what we learned last week make a word2vec neural net with a $5$ node tall hidden layer.\n",
    "\n",
    "Use `rmsprop` as your `optimizer`, `binary_crossentropy` as your `loss`, and `accuracy` as your `metrics`.\n",
    "\n",
    "Also train for at least `1000` `epochs`, with a batch size of `12`.\n",
    "\n",
    "_Hint: to make a layer with identity activation just don't include the `activation` = argument_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams = [('king','kingdom'),('queen','kingdom'),('king','palace'),('queen','palace'),('king','royal'),\n",
    "            ('queen','royal'),('king','George'),('queen','Mary'),('man','rice'),('woman','rice'),\n",
    "            ('man','farmer'),('woman','farmer'),('man','house'),('woman','house'),('man','George'),\n",
    "            ('woman','Mary')]\n",
    "\n",
    "word_index = {'George':0, 'Mary':1, 'farmer':2, 'house':3, 'kingdom':4,\n",
    "                 'king':5, 'man':6, 'palace':7, 'queen':8, 'rice':9, 'royal':10,\n",
    "                 'woman':11}\n",
    "\n",
    "skipgrams = [(word_index[gram[0]],word_index[gram[1]]) for gram in skipgrams]\n",
    "\n",
    "reverse_index = {i:word for word,i in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(word_index.keys()),len(skipgrams)))\n",
    "y = np.zeros((len(word_index.keys()),len(skipgrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(skipgrams)):\n",
    "    gram = skipgrams[j]\n",
    "    X[gram[1],j] = 1\n",
    "    y[gram[0],j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need these\n",
    "#from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having problem with Tensorflow   \n",
    "Run the Following in your command Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall tensorflow\n",
    "#!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code\n",
    "### Call your neural net model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Word Embedding\n",
    "Now we need to get the weight matrix. We didn't review how to do this in the `Neural Networks` folder so let's see how to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights.append(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the 0th entry of the 0th entry in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(weights[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting to 2-dimensions\n",
    "Now we can look at the word embedding in two dimensions using a standard dimension reduction technique like PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit=pca.fit_transform(weights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(fit[:,0],fit[:,1])\n",
    "\n",
    "for i in range(len(word_index.keys())):\n",
    "    plt.text(fit[i,0],fit[i,1],reverse_index[i], fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with this notebook, I want to pause and show you a nice interactive web app that gives a good intuition for Word2Vec, https://ronxin.github.io/wevi/. Go to that app and play around for a bit before finishing this notebook.\n",
    "\n",
    "## Training Word2Vec is Costly\n",
    "Before moving on to the next notebook let's end with this final demonstration.\n",
    "\n",
    "Program a loop to count the number of skip-grams that would result from the imdb data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print how many skip-grams there were"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That's a lot of data!\n",
    "\n",
    "To make things worse (from your laptop's perspective) in the imdb example $M =10,000$ and a standard $N$ is $300$  (based on original paper). That means we'd need to find weights for $10,000\\times 300 = 3,000,000$  weights twice. (Good thing we have a lot of data).\n",
    "\n",
    "So training your own Word2Vec embedding comes with a large start up cost compared to everything else we've done in the program.\n",
    "\n",
    "That's why many projects don't start with the training of a custom Word2Vec embedding, but first either try some of the older techniques we've learned or use a pretrained Word2Vec embedding, the topic of Next Week's Class!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
