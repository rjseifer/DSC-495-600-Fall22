{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Word2Vec](#Word2Vec)\n",
    "\n",
    "1. [Introduction](#Introduction)   \n",
    "2. [References](#References)   \n",
    "3. [The Idea Behind the Approach - Using Context Clues](#The-Idea-Behind-the-Approach---Using-Context-Clues)     \n",
    "4. [Searching for Context Clues: Skip-Grams](#Searching-for-Context-Clues:-Skip-Grams)   \n",
    "5. [The Neural Network](#The-Neural-Network)    \n",
    "6. [Training Word2Vec is Costly](#Training-Word2Vec-is-Costly)   \n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "## Introduction\n",
    "In this section we'll be diving into how neural networks can be used to represent NLP data in what are known as embeddings. They are so called because we are taking higher dimensional data and finding clever ways to embed it into a smaller dimensional space.\n",
    "\n",
    "By clever I mean that we'll be finding ways to embed that preserve some inherent nature of the data, for instance in a way that makes sense given a word's meaning in a piece of text. For example, a good word embedding should probably group the words \"apple\" and \"pears\" together (depending on the data set of course).\n",
    "\n",
    "\n",
    "The first feed forward neural network based word embedding we'll touch on is Word2Vec. Word2Vec was an approach developed by Mikolov et. al. across two papers in 2013 (https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546).\n",
    "\n",
    "The key idea behind this approach is to look at the words that occur next to the word you're interested in across your corpus, an idea known as looking at the context surround the target word.\n",
    "\n",
    "In this notebook we'll learn how to implement Word2Vec using an algorithm known as skip-gram with negative sampling (SGNS).\n",
    "\n",
    "## References\n",
    "\n",
    "Notably, Mikolov et. al.'s papers are a bit hard to decipher, so we'll actually be working off of a collection of other sources that are much more reader-friendly (i.e. I can understand them).\n",
    "\n",
    "This notebook will build heavily off of the following sources:\n",
    "\n",
    "* https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "* https://www.tensorflow.org/tutorials/text/word2vec\n",
    "* https://arxiv.org/abs/1411.2738\n",
    "* https://www.youtube.com/watch?v=D-ekE-Wlcds\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll import these, but I don't know how much \n",
    "## we'll use them\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind the Approach - Using Context Clues\n",
    "Before diving into the nitty-gritty lets dive into the idea behind the approach.\n",
    "\n",
    "\n",
    "\n",
    " You shall know a word by the company it keeps \n",
    "\n",
    "J.R. Firth\n",
    "\n",
    "\n",
    "At the heart of Word2Vec is the desire to learn the \"meaning\" of a word using context clues. Let's look at a simple example. Imagine you've picked up the hottest young adult novel set in a far off dystopian future. As many such novels do the author has created a \"fun\" future slang for their dystopian future and, being from the present, the words are unfamiliar to you. You come upon the following sentence, \"After work he needed to deposit his weekly pay at the kakoonahole.\"\n",
    "\n",
    "I would be shocked if you have heard the word kakoonahole before, but you probably have a rough idea that the author is using this word to represent a banking establishment of some kind.\n",
    "\n",
    "What gave that away? Neighboring words like \"weekly pay\" and \"deposit\".\n",
    "\n",
    "This is precisely the idea behind Word2Vec.\n",
    "\n",
    "With Word2Vec we will see how we can use a very simple feed forward neural network architecture (a single hidden layer) to produce representations of words as short dense vectors, as opposed to the standard long sparse vectors we've used up to this point (one-hot encodings, frequency vectors, tf-idf vectors).\n",
    "\n",
    "Importantly, these short dense vectors have been demonstrated to provide intuitive results that outperform previous techniques (like LSA) at certain tasks.\n",
    "\n",
    "## Searching for Context Clues: Skip-Grams\n",
    "When we use the phrase \"context clues\", we mean the words surrounding the word in which we have an interest. We can quantify the context words surrounding our target word using skip-grams.\n",
    "\n",
    "Consider this sample sentence.\n",
    "\n",
    "My cat sits in the sun\n",
    "\n",
    "For skip-grams we focus on a single word, and the window around that word. In this example let's choose a window size of 2. To create a collection of skip-grams you choose a target word, let's say it is sits, and look at all the words within the window size on either side. The skip-grams are then the collection of target word-window word pairings. For sits and size 2 this gives (sits, my), (sits, cat), (sits, in), (sits, the).\n",
    "\n",
    "Before moving on let's do a short practice to register understanding. Write down the skip-grams for the word cat.\n",
    "\n",
    "ANSWER\n",
    "(cat, My), (cat, sits), (cat, in).\n",
    "\n",
    "### Making skip-grams in keras\n",
    "We can use keras to quickly make skip-grams for us, let's see how!\n",
    "\n",
    "Preparing the data\n",
    "First you'll need to turn your text into a list of indices, like the imdb data set from the neural networks folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'cat', 'sits', 'in', 'the', 'sun']\n"
     ]
    }
   ],
   "source": [
    "## First we'll tokenize our data using simple string functions\n",
    "sample_sentence = \"My cat sits in the sun\"\n",
    "\n",
    "tokens = list(sample_sentence.lower().split())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'cat': 2, 'sits': 3, 'in': 4, 'the': 5, 'sun': 6}\n"
     ]
    }
   ],
   "source": [
    "# now we create a word index dictionary\n",
    "word_index = {}\n",
    "i = 1\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in word_index.keys():\n",
    "        word_index[word] = i\n",
    "        i = i + 1\n",
    "        \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'my', 2: 'cat', 3: 'sits', 4: 'in', 5: 'the', 6: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "# now we create a reverse index as well\n",
    "reverse_word_index = {i: word for word,i in word_index.items()}\n",
    "\n",
    "print(reverse_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# we can now create a sequence for our sentence like so\n",
    "sample_sequence = [word_index[word] for word in tokens]\n",
    "print(sample_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the skip-grams\n",
    "Now that we have a sequence for our sentence we can use `keras` to create a set of skip-grams for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large you want your windows to be\n",
    "window_size = 2\n",
    "\n",
    "# how many words are in your vocabulary?\n",
    "vocabulary_size = len(word_index.keys())\n",
    "\n",
    "# ignore the negative_samples argument for now\n",
    "# more on that later\n",
    "positive_skip_grams, _ = sequence.skipgrams(sample_sequence, \n",
    "                                  vocabulary_size=vocabulary_size,\n",
    "                                  window_size=window_size,\n",
    "                                  negative_samples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4],\n",
       " [3, 1],\n",
       " [6, 4],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [2, 1],\n",
       " [2, 3],\n",
       " [5, 3],\n",
       " [4, 6],\n",
       " [4, 5],\n",
       " [3, 4],\n",
       " [3, 2],\n",
       " [5, 6],\n",
       " [4, 2],\n",
       " [5, 4],\n",
       " [4, 3],\n",
       " [6, 5],\n",
       " [3, 5]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence was:\n",
      "My cat sits in the sun\n",
      "######################\n",
      "The skip-grams are:\n",
      "cat in\n",
      "sits my\n",
      "sun in\n",
      "my cat\n",
      "my sits\n",
      "cat my\n",
      "cat sits\n",
      "the sits\n",
      "in sun\n",
      "in the\n",
      "sits in\n",
      "sits cat\n",
      "the sun\n",
      "in cat\n",
      "the in\n",
      "in sits\n",
      "sun the\n",
      "sits the\n"
     ]
    }
   ],
   "source": [
    "print(\"The sentence was:\")\n",
    "print(sample_sentence)\n",
    "\n",
    "print(\"######################\")\n",
    "\n",
    "print(\"The skip-grams are:\")\n",
    "for item in positive_skip_grams:\n",
    "    print(reverse_word_index[item[0]],reverse_word_index[item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code\n",
    "Use the next few code chunks to read in the imdb data set from `keras`. The practice by calculating the skip-grams for the a couple of reviews from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melissangamini/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/melissangamini/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "(imdb_train,y_train_imdb), (imdb_test,y_test_imdb) = imdb.load_data(num_words=n, \n",
    "                                                                                seed=440,\n",
    "                                                                                index_from=3)\n",
    "\n",
    "# word_index is a dictionary that maps each word to it's index\n",
    "imdb_word_index = imdb.get_word_index()\n",
    "\n",
    "# We now adjust the indices according to the coding presented above\n",
    "imdb_word_index = {key:(value+3) for key,value in imdb_word_index.items()}\n",
    "\n",
    "imdb_word_index[\"<PAD>\"] = 0\n",
    "imdb_word_index[\"<START>\"] = 1\n",
    "imdb_word_index[\"<UNKNOWN WORD>\"] = 2\n",
    "imdb_word_index[\"<UNUSED WORD>\"] = 3\n",
    "\n",
    "imdb_reverse_index = dict([(value,key) for (key,value) in imdb_word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence was:\n",
      "<START> i watched this movie on march 21 this year must say disappointment but much better than <UNKNOWN WORD> plot is hackneyed tells about <UNKNOWN WORD> who lives with his father wife and his little brother the movie opens when he saves a bride anyway <UNKNOWN WORD> <UNKNOWN WORD> makes a plot to kill his young brother he makes a plan by sending few man they come to a school <UNKNOWN WORD> to be <UNKNOWN WORD> friends kill that kid his father throws him out of the house then later comes back he and <UNKNOWN WORD> go to <UNKNOWN WORD> to find him sunny gives a good performance <UNKNOWN WORD> was annoying at best <UNKNOWN WORD> is wasted <UNKNOWN WORD> did good <UNKNOWN WORD> was wasted <UNKNOWN WORD> was wasted but looked cute the kid which played <UNKNOWN WORD> brother in the movie was cute too sad he had to get his character killed the girl was cute but was annoying the other kid did good <UNKNOWN WORD> did good <UNKNOWN WORD> was adequate <UNKNOWN WORD> and <UNKNOWN WORD> did good the cinematography is excellent in both india and <UNKNOWN WORD> script is weak but has a few good dialogs also drags the movie the music was alright i only liked one song <UNKNOWN WORD> <UNKNOWN WORD> the lyrics of that song was good the other songs were forgettable don't watch this rating 3 10\n",
      "######################\n",
      "The skip-grams are:\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> anyway\n",
      "about <UNKNOWN WORD>\n",
      "3 this\n",
      "anyway <UNKNOWN WORD>\n",
      "to <UNKNOWN WORD>\n",
      "also good\n",
      "brother makes\n",
      "of lyrics\n",
      "then comes\n",
      "<UNKNOWN WORD> did\n",
      "did wasted\n",
      "song <UNKNOWN WORD>\n",
      "the music\n",
      "back comes\n",
      "was good\n",
      "this on\n",
      "weak but\n",
      "wife father\n",
      "wasted <UNKNOWN WORD>\n",
      "was the\n",
      "alright only\n",
      "his throws\n",
      "be <UNKNOWN WORD>\n",
      "cute sad\n",
      "good dialogs\n",
      "performance good\n",
      "both india\n",
      "<UNKNOWN WORD> good\n",
      "annoying the\n",
      "<UNKNOWN WORD> annoying\n",
      "plot kill\n",
      "the other\n",
      "the was\n",
      "and wife\n",
      "music the\n",
      "brother his\n",
      "brother movie\n",
      "is <UNKNOWN WORD>\n",
      "did <UNKNOWN WORD>\n",
      "songs forgettable\n",
      "disappointment much\n",
      "his father\n",
      "to they\n",
      "man few\n",
      "the of\n",
      "also drags\n",
      "annoying but\n",
      "best is\n",
      "father his\n",
      "<UNKNOWN WORD> one\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "plan sending\n",
      "dialogs also\n",
      "was movie\n",
      "script <UNKNOWN WORD>\n",
      "character his\n",
      "sad he\n",
      "man they\n",
      "of him\n",
      "few man\n",
      "get had\n",
      "wasted was\n",
      "annoying at\n",
      "forgettable watch\n",
      "man come\n",
      "was <UNKNOWN WORD>\n",
      "in brother\n",
      "hackneyed plot\n",
      "was girl\n",
      "few by\n",
      "is excellent\n",
      "good was\n",
      "and back\n",
      "makes a\n",
      "was wasted\n",
      "that song\n",
      "to plot\n",
      "but was\n",
      "too was\n",
      "tells <UNKNOWN WORD>\n",
      "he had\n",
      "a plan\n",
      "a good\n",
      "be friends\n",
      "was performance\n",
      "kid the\n",
      "his little\n",
      "good gives\n",
      "to his\n",
      "with his\n",
      "a performance\n",
      "good performance\n",
      "<UNKNOWN WORD> was\n",
      "comes he\n",
      "back he\n",
      "alright music\n",
      "looked wasted\n",
      "house later\n",
      "this must\n",
      "<UNKNOWN WORD> better\n",
      "must say\n",
      "wasted is\n",
      "weak has\n",
      "house then\n",
      "bride anyway\n",
      "the was\n",
      "good did\n",
      "good did\n",
      "by sending\n",
      "by a\n",
      "<UNKNOWN WORD> go\n",
      "wasted <UNKNOWN WORD>\n",
      "movie when\n",
      "lyrics that\n",
      "year say\n",
      "makes plan\n",
      "liked i\n",
      "a few\n",
      "was good\n",
      "little the\n",
      "lives his\n",
      "the also\n",
      "has weak\n",
      "songs other\n",
      "opens when\n",
      "father and\n",
      "the good\n",
      "which played\n",
      "was cute\n",
      "his kid\n",
      "house of\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "he <UNKNOWN WORD>\n",
      "this watch\n",
      "plan a\n",
      "i this\n",
      "was <UNKNOWN WORD>\n",
      "cute but\n",
      "young he\n",
      "than <UNKNOWN WORD>\n",
      "go <UNKNOWN WORD>\n",
      "anyway <UNKNOWN WORD>\n",
      "india in\n",
      "<UNKNOWN WORD> was\n",
      "liked song\n",
      "with lives\n",
      "cute too\n",
      "the which\n",
      "in india\n",
      "his with\n",
      "good cinematography\n",
      "plot to\n",
      "his young\n",
      "is the\n",
      "excellent cinematography\n",
      "i watched\n",
      "<UNKNOWN WORD> is\n",
      "<UNKNOWN WORD> was\n",
      "looked the\n",
      "dialogs drags\n",
      "to his\n",
      "little and\n",
      "the good\n",
      "much but\n",
      "brother played\n",
      "the drags\n",
      "brother the\n",
      "with father\n",
      "the songs\n",
      "wife his\n",
      "but weak\n",
      "friends be\n",
      "say but\n",
      "disappointment must\n",
      "plot is\n",
      "cute was\n",
      "<UNKNOWN WORD> he\n",
      "house the\n",
      "the <UNKNOWN WORD>\n",
      "watched movie\n",
      "rating this\n",
      "dialogs few\n",
      "rating 3\n",
      "kid that\n",
      "girl cute\n",
      "makes he\n",
      "kill kid\n",
      "throws his\n",
      "little brother\n",
      "little his\n",
      "wasted was\n",
      "this 21\n",
      "watch forgettable\n",
      "had get\n",
      "a plot\n",
      "kid did\n",
      "was <UNKNOWN WORD>\n",
      "but is\n",
      "to a\n",
      "movie was\n",
      "and india\n",
      "bride a\n",
      "movie the\n",
      "him throws\n",
      "gives good\n",
      "tells is\n",
      "to to\n",
      "movie brother\n",
      "was at\n",
      "annoying <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> about\n",
      "best annoying\n",
      "annoying was\n",
      "<UNKNOWN WORD> song\n",
      "was the\n",
      "killed character\n",
      "good <UNKNOWN WORD>\n",
      "young kill\n",
      "other the\n",
      "to had\n",
      "<UNKNOWN WORD> was\n",
      "was annoying\n",
      "he too\n",
      "good song\n",
      "weak is\n",
      "<UNKNOWN WORD> plot\n",
      "annoying other\n",
      "performance was\n",
      "good was\n",
      "much better\n",
      "10 3\n",
      "sad too\n",
      "killed his\n",
      "were forgettable\n",
      "of house\n",
      "get his\n",
      "much disappointment\n",
      "did <UNKNOWN WORD>\n",
      "<START> i\n",
      "which the\n",
      "find him\n",
      "who lives\n",
      "both in\n",
      "year this\n",
      "but has\n",
      "him father\n",
      "were other\n",
      "march on\n",
      "to find\n",
      "hackneyed about\n",
      "but looked\n",
      "and father\n",
      "and script\n",
      "girl was\n",
      "than plot\n",
      "with who\n",
      "this movie\n",
      "throws out\n",
      "3 rating\n",
      "bride <UNKNOWN WORD>\n",
      "than better\n",
      "he opens\n",
      "they come\n",
      "killed girl\n",
      "and <UNKNOWN WORD>\n",
      "wasted was\n",
      "that of\n",
      "dialogs good\n",
      "did good\n",
      "<UNKNOWN WORD> be\n",
      "him gives\n",
      "at was\n",
      "songs were\n",
      "come man\n",
      "21 this\n",
      "adequate <UNKNOWN WORD>\n",
      "much than\n",
      "<UNKNOWN WORD> wasted\n",
      "<UNKNOWN WORD> brother\n",
      "<UNKNOWN WORD> the\n",
      "get to\n",
      "plan by\n",
      "back and\n",
      "kill plot\n",
      "both excellent\n",
      "few has\n",
      "lyrics of\n",
      "did kid\n",
      "don't forgettable\n",
      "cute looked\n",
      "was the\n",
      "plot hackneyed\n",
      "the killed\n",
      "in <UNKNOWN WORD>\n",
      "in movie\n",
      "to come\n",
      "only one\n",
      "his get\n",
      "tells about\n",
      "the opens\n",
      "was but\n",
      "<UNKNOWN WORD> in\n",
      "a makes\n",
      "that was\n",
      "the was\n",
      "on 21\n",
      "<UNKNOWN WORD> to\n",
      "brother the\n",
      "watch this\n",
      "<UNKNOWN WORD> lives\n",
      "come a\n",
      "movie opens\n",
      "i was\n",
      "plot makes\n",
      "throws him\n",
      "the movie\n",
      "played brother\n",
      "forgettable songs\n",
      "wasted <UNKNOWN WORD>\n",
      "good also\n",
      "other good\n",
      "a by\n",
      "<UNKNOWN WORD> performance\n",
      "wife his\n",
      "other kid\n",
      "that lyrics\n",
      "to get\n",
      "the looked\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "was cute\n",
      "a <UNKNOWN WORD>\n",
      "wasted <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> is\n",
      "<UNKNOWN WORD> adequate\n",
      "then the\n",
      "a come\n",
      "the girl\n",
      "a anyway\n",
      "march this\n",
      "to go\n",
      "gives him\n",
      "to school\n",
      "cute girl\n",
      "script is\n",
      "was music\n",
      "of the\n",
      "<UNKNOWN WORD> go\n",
      "<UNKNOWN WORD> wasted\n",
      "movie the\n",
      "sunny a\n",
      "good <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> at\n",
      "back later\n",
      "a to\n",
      "of song\n",
      "year must\n",
      "by few\n",
      "brother he\n",
      "song of\n",
      "liked only\n",
      "to <UNKNOWN WORD>\n",
      "to <UNKNOWN WORD>\n",
      "him sunny\n",
      "and <UNKNOWN WORD>\n",
      "a but\n",
      "annoying best\n",
      "is tells\n",
      "killed the\n",
      "good the\n",
      "<UNKNOWN WORD> a\n",
      "<UNKNOWN WORD> and\n",
      "this rating\n",
      "<UNKNOWN WORD> good\n",
      "did other\n",
      "the kid\n",
      "<UNKNOWN WORD> good\n",
      "did <UNKNOWN WORD>\n",
      "has a\n",
      "<UNKNOWN WORD> song\n",
      "10 rating\n",
      "but cute\n",
      "lives with\n",
      "was wasted\n",
      "<UNKNOWN WORD> is\n",
      "<UNKNOWN WORD> bride\n",
      "was <UNKNOWN WORD>\n",
      "by plan\n",
      "he brother\n",
      "<UNKNOWN WORD> and\n",
      "<UNKNOWN WORD> wasted\n",
      "watch don't\n",
      "sunny gives\n",
      "was but\n",
      "but was\n",
      "movie on\n",
      "has few\n",
      "who with\n",
      "alright was\n",
      "the brother\n",
      "go to\n",
      "kill young\n",
      "songs the\n",
      "out him\n",
      "<UNKNOWN WORD> find\n",
      "to a\n",
      "father him\n",
      "who about\n",
      "brother young\n",
      "on movie\n",
      "out throws\n",
      "cute the\n",
      "this don't\n",
      "one <UNKNOWN WORD>\n",
      "his brother\n",
      "to be\n",
      "<UNKNOWN WORD> script\n",
      "be to\n",
      "adequate and\n",
      "good did\n",
      "the the\n",
      "wasted did\n",
      "performance a\n",
      "but a\n",
      "<UNKNOWN WORD> did\n",
      "<UNKNOWN WORD> lyrics\n",
      "script and\n",
      "man sending\n",
      "he makes\n",
      "that his\n",
      "say disappointment\n",
      "kid cute\n",
      "was the\n",
      "also dialogs\n",
      "his to\n",
      "later house\n",
      "few sending\n",
      "<UNKNOWN WORD> best\n",
      "must year\n",
      "the lyrics\n",
      "drags also\n",
      "a gives\n",
      "be <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> wasted\n",
      "drags the\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "his and\n",
      "song that\n",
      "song <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> is\n",
      "this watched\n",
      "rating 10\n",
      "a he\n",
      "<UNKNOWN WORD> to\n",
      "must disappointment\n",
      "was good\n",
      "played which\n",
      "father throws\n",
      "<UNKNOWN WORD> friends\n",
      "his lives\n",
      "sunny find\n",
      "movie drags\n",
      "and both\n",
      "<UNKNOWN WORD> than\n",
      "makes a\n",
      "don't this\n",
      "come to\n",
      "then house\n",
      "brother <UNKNOWN WORD>\n",
      "comes later\n",
      "<UNKNOWN WORD> did\n",
      "cute was\n",
      "out the\n",
      "movie this\n",
      "<UNKNOWN WORD> good\n",
      "school to\n",
      "<UNKNOWN WORD> good\n",
      "21 on\n",
      "he young\n",
      "good a\n",
      "character the\n",
      "one song\n",
      "to <UNKNOWN WORD>\n",
      "did the\n",
      "song one\n",
      "of that\n",
      "and go\n",
      "<UNKNOWN WORD> wasted\n",
      "is <UNKNOWN WORD>\n",
      "music alright\n",
      "he back\n",
      "they man\n",
      "excellent both\n",
      "makes <UNKNOWN WORD>\n",
      "was wasted\n",
      "father with\n",
      "cute kid\n",
      "and <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> school\n",
      "good <UNKNOWN WORD>\n",
      "lyrics <UNKNOWN WORD>\n",
      "he to\n",
      "watched this\n",
      "music was\n",
      "must this\n",
      "cute movie\n",
      "makes brother\n",
      "but much\n",
      "movie in\n",
      "saves he\n",
      "song was\n",
      "other were\n",
      "played <UNKNOWN WORD>\n",
      "and adequate\n",
      "is plot\n",
      "only i\n",
      "him out\n",
      "is wasted\n",
      "when he\n",
      "which <UNKNOWN WORD>\n",
      "liked one\n",
      "drags dialogs\n",
      "the was\n",
      "his kill\n",
      "better much\n",
      "later back\n",
      "a school\n",
      "the cinematography\n",
      "watch rating\n",
      "did and\n",
      "kill to\n",
      "but better\n",
      "india <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> and\n",
      "were songs\n",
      "lives who\n",
      "the other\n",
      "good did\n",
      "did <UNKNOWN WORD>\n",
      "about who\n",
      "that kid\n",
      "few a\n",
      "at best\n",
      "character get\n",
      "good a\n",
      "adequate <UNKNOWN WORD>\n",
      "movie cute\n",
      "and did\n",
      "other the\n",
      "<UNKNOWN WORD> good\n",
      "kid kill\n",
      "kid his\n",
      "they few\n",
      "i liked\n",
      "was too\n",
      "was alright\n",
      "that friends\n",
      "good did\n",
      "lyrics the\n",
      "watched i\n",
      "<UNKNOWN WORD> which\n",
      "looked cute\n",
      "comes then\n",
      "a saves\n",
      "to to\n",
      "to <UNKNOWN WORD>\n",
      "the out\n",
      "girl the\n",
      "forgettable don't\n",
      "at <UNKNOWN WORD>\n",
      "sunny him\n",
      "<UNKNOWN WORD> anyway\n",
      "but disappointment\n",
      "don't were\n",
      "is best\n",
      "kid father\n",
      "i only\n",
      "<UNKNOWN WORD> good\n",
      "this 3\n",
      "did good\n",
      "to school\n",
      "his father\n",
      "<UNKNOWN WORD> played\n",
      "throws father\n",
      "on this\n",
      "the movie\n",
      "was adequate\n",
      "say year\n",
      "friends <UNKNOWN WORD>\n",
      "saves a\n",
      "<UNKNOWN WORD> makes\n",
      "few good\n",
      "too cute\n",
      "when saves\n",
      "good <UNKNOWN WORD>\n",
      "alright i\n",
      "is weak\n",
      "was that\n",
      "<UNKNOWN WORD> to\n",
      "only alright\n",
      "him of\n",
      "brother his\n",
      "march 21\n",
      "he a\n",
      "cinematography is\n",
      "is but\n",
      "was annoying\n",
      "the annoying\n",
      "the the\n",
      "21 year\n",
      "cute was\n",
      "the little\n",
      "anyway bride\n",
      "gives sunny\n",
      "<UNKNOWN WORD> <UNKNOWN WORD>\n",
      "is script\n",
      "bride saves\n",
      "who <UNKNOWN WORD>\n",
      "the in\n",
      "the then\n",
      "cinematography good\n",
      "good was\n",
      "one only\n",
      "kid which\n",
      "was but\n",
      "good kid\n",
      "father his\n",
      "of the\n",
      "and <UNKNOWN WORD>\n",
      "sending plan\n",
      "the kid\n",
      "in is\n",
      "was the\n",
      "good the\n",
      "<UNKNOWN WORD> india\n",
      "sad had\n",
      "movie watched\n",
      "his killed\n",
      "had sad\n",
      "the <UNKNOWN WORD>\n",
      "friends that\n",
      "good <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> who\n",
      "the movie\n",
      "excellent is\n",
      "out of\n",
      "they to\n",
      "the cute\n",
      "watched <START>\n",
      "music movie\n",
      "when movie\n",
      "<START> watched\n",
      "annoying was\n",
      "school <UNKNOWN WORD>\n",
      "this march\n",
      "in both\n",
      "i <START>\n",
      "is hackneyed\n",
      "best at\n",
      "21 march\n",
      "too sad\n",
      "when opens\n",
      "young his\n",
      "to him\n",
      "character killed\n",
      "tells hackneyed\n",
      "but say\n",
      "good <UNKNOWN WORD>\n",
      "one liked\n",
      "both and\n",
      "he a\n",
      "on march\n",
      "kill his\n",
      "<UNKNOWN WORD> did\n",
      "and his\n",
      "drags movie\n",
      "<UNKNOWN WORD> the\n",
      "<UNKNOWN WORD> did\n",
      "brother little\n",
      "<UNKNOWN WORD> a\n",
      "father wife\n",
      "also the\n",
      "comes back\n",
      "the was\n",
      "to kill\n",
      "sad cute\n",
      "disappointment but\n",
      "has but\n",
      "makes <UNKNOWN WORD>\n",
      "is <UNKNOWN WORD>\n",
      "a <UNKNOWN WORD>\n",
      "and little\n",
      "plot <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> to\n",
      "good other\n",
      "about tells\n",
      "march movie\n",
      "friends kill\n",
      "best <UNKNOWN WORD>\n",
      "him to\n",
      "a he\n",
      "did good\n",
      "gives a\n",
      "the brother\n",
      "is in\n",
      "movie the\n",
      "<UNKNOWN WORD> tells\n",
      "kid the\n",
      "sending by\n",
      "anyway a\n",
      "other did\n",
      "saves when\n",
      "hackneyed is\n",
      "wasted <UNKNOWN WORD>\n",
      "find <UNKNOWN WORD>\n",
      "plot a\n",
      "did <UNKNOWN WORD>\n",
      "kid other\n",
      "better <UNKNOWN WORD>\n",
      "<UNKNOWN WORD> to\n",
      "opens he\n",
      "lives <UNKNOWN WORD>\n",
      "his character\n",
      "i alright\n",
      "looked but\n",
      "come they\n",
      "the character\n",
      "say must\n",
      "find to\n",
      "song liked\n",
      "had to\n",
      "was <UNKNOWN WORD>\n",
      "find sunny\n",
      "but cute\n",
      "movie the\n",
      "few they\n",
      "excellent in\n",
      "is cinematography\n",
      "cute but\n",
      "3 10\n",
      "his wife\n",
      "father kid\n",
      "played kid\n",
      "kid played\n",
      "the is\n",
      "<UNKNOWN WORD> be\n",
      "better than\n",
      "too he\n",
      "cinematography excellent\n",
      "a makes\n",
      "and he\n",
      "kill friends\n",
      "<UNKNOWN WORD> and\n",
      "school to\n",
      "hackneyed tells\n",
      "good few\n",
      "which kid\n",
      "brother in\n",
      "opens the\n",
      "but wasted\n",
      "india both\n",
      "<UNKNOWN WORD> makes\n",
      "later then\n",
      "but annoying\n",
      "go and\n",
      "<UNKNOWN WORD> adequate\n",
      "rating watch\n",
      "the house\n",
      "other songs\n",
      "a has\n",
      "the of\n",
      "forgettable were\n",
      "<UNKNOWN WORD> kill\n",
      "<UNKNOWN WORD> was\n",
      "plan makes\n",
      "that kill\n",
      "he saves\n",
      "script weak\n",
      "the movie\n",
      "about hackneyed\n",
      "school a\n",
      "sending man\n",
      "his brother\n",
      "<UNKNOWN WORD> was\n",
      "india and\n",
      "movie march\n",
      "young brother\n",
      "at annoying\n",
      "then later\n",
      "wasted looked\n",
      "this year\n",
      "disappointment say\n",
      "go <UNKNOWN WORD>\n",
      "a good\n",
      "adequate was\n",
      "had he\n",
      "later comes\n",
      "than much\n",
      "wife and\n",
      "makes plot\n",
      "weak script\n",
      "year 21\n",
      "he comes\n",
      "girl killed\n",
      "performance <UNKNOWN WORD>\n",
      "were don't\n",
      "of out\n",
      "only liked\n",
      "don't watch\n",
      "was cute\n",
      "the did\n",
      "plot than\n",
      "a bride\n",
      "his to\n",
      "he and\n",
      "was <UNKNOWN WORD>\n",
      "is <UNKNOWN WORD>\n",
      "did good\n",
      "was song\n",
      "him find\n",
      "better but\n",
      "<UNKNOWN WORD> did\n",
      "saves bride\n",
      "kill that\n",
      "kid good\n",
      "sending few\n",
      "cinematography the\n",
      "wasted but\n",
      "did good\n",
      "was i\n",
      "get character\n",
      "he sad\n",
      "a sunny\n",
      "to he\n",
      "his wife\n",
      "but was\n",
      "his that\n",
      "opens movie\n",
      "movie music\n",
      "kill <UNKNOWN WORD>\n",
      "he when\n",
      "a to\n",
      "song good\n",
      "in the\n",
      "this i\n",
      "other annoying\n",
      "in excellent\n",
      "did <UNKNOWN WORD>\n",
      "good <UNKNOWN WORD>\n",
      "few dialogs\n"
     ]
    }
   ],
   "source": [
    "### You Code Here\n",
    "positive_skip_grams_71, _ = sequence.skipgrams(\n",
    "                                  imdb_train[71], \n",
    "                                  vocabulary_size=n,\n",
    "                                  window_size=window_size,\n",
    "                                  negative_samples=0)\n",
    "\n",
    "print(\"The sentence was:\")\n",
    "print(\" \".join([imdb_reverse_index[i] for i in imdb_train[71]]))\n",
    "\n",
    "print(\"######################\")\n",
    "\n",
    "print(\"The skip-grams are:\")\n",
    "for item in positive_skip_grams_71:\n",
    "    print(imdb_reverse_index[item[0]],imdb_reverse_index[item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "The neural network used for __Word2Vec__, as previously mentioned, has a very simple architecture and we will introduce it now.\n",
    "\n",
    "Suppose you wish to represent a vocabulary of $M$ unique words/tokens from your corpus. You then build a network with `M` input nodes, one corresponding to each word in the vocabulary, a single hidden layer with `N` nodes where , and an output layer with M nodes, again corresponding to each word in the vocabulary.\n",
    "\n",
    "Its architecture looks like this \n",
    "\n",
    "As you can see from the architecture, the activation from the input layer to the hidden layer is the identity function and the activation from the hidden layer to the output layer is the softmax function, which is defined for a vector $z$ with $K$ entries as:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum^{K}_{j=1}e^{z_j}},\n",
    "$$\n",
    " \n",
    "and thus transforms the output from nodes of real numbers to a probability distribution.\n",
    "\n",
    "### Why The Softmax?\n",
    "The genius of the __Word2Vec__ approach was that it turned the semantics problem, \"what does word $w$ mean?\" into a context problem, \"what is the context of $w$?\" is then turned into a multiclass classification problem.\n",
    "\n",
    "With the above neural net the goal of the network is to model the probability that word \n",
    " is a contextual word (in the skip-gram window) of a given target word \n",
    ", i.e. we're modeling the conditional probability:\n",
    "\n",
    "$$\n",
    "p(w_j|w_t)\\sim \\sigma(O)_j= e^{O_j}/\\sum^{K}_{i=1}e^{O_i}\n",
    "$$\n",
    "\n",
    "where I'm lazily letting $O_i$ denote the output of the weighted sum of the hidden layer nodes at output node $i$.\n",
    "\n",
    "### What is the Training Data?\n",
    "The training data for this network is produced from the skipgrams. An $X,y$  pair in the training set would be as follows. $X$ would be a one hot encoded vector where all $M$ entries are $0$  except for the entry corresponding to the target word of interest. $y$ is a one hot encoded vector where all $M$ entries are $0$ except for the entry corresponding to the context word.\n",
    "\n",
    "### Weights are Where it's at\n",
    "However, we don't care at all about using the network to make predictions, we just want the weight matrices that result.\n",
    "\n",
    "Let $W$ be the $N\\times M$  trained weight matrix for the input layer into the hidden layer, and let $x_i(M\\times1)$ be a one-hot encoded vector corresponding to word $w_i$, then the word2vec embedding of $w_i$ is simply $Ww_i$ which is the $i$^{th} column of $W$ .\n",
    "\n",
    "You Code, A Very Simple Example\n",
    "I've coded up the skip grams for a very simple example below and created the $X$ and $y$ for you. Using what we learned last week make a word2vec neural net with a $5$ node tall hidden layer.\n",
    "\n",
    "Use `rmsprop` as your `optimizer`, `binary_crossentropy` as your `loss`, and `accuracy` as your `metrics`.\n",
    "\n",
    "Also train for at least `1000` `epochs`, with a batch size of `12`.\n",
    "\n",
    "_Hint: to make a layer with identity activation just don't include the `activation` = argument_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams = [('king','kingdom'),('queen','kingdom'),('king','palace'),('queen','palace'),('king','royal'),\n",
    "            ('queen','royal'),('king','George'),('queen','Mary'),('man','rice'),('woman','rice'),\n",
    "            ('man','farmer'),('woman','farmer'),('man','house'),('woman','house'),('man','George'),\n",
    "            ('woman','Mary')]\n",
    "\n",
    "word_index = {'George':0, 'Mary':1, 'farmer':2, 'house':3, 'kingdom':4,\n",
    "                 'king':5, 'man':6, 'palace':7, 'queen':8, 'rice':9, 'royal':10,\n",
    "                 'woman':11}\n",
    "\n",
    "skipgrams = [(word_index[gram[0]],word_index[gram[1]]) for gram in skipgrams]\n",
    "\n",
    "reverse_index = {i:word for word,i in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(word_index.keys()),len(skipgrams)))\n",
    "y = np.zeros((len(word_index.keys()),len(skipgrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(skipgrams)):\n",
    "    gram = skipgrams[j]\n",
    "    X[gram[1],j] = 1\n",
    "    y[gram[0],j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need these\n",
    "#from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having problem with Tensorflow   \n",
    "Run the Following in your command Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall tensorflow\n",
    "#!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You code\n",
    "\n",
    "# we first make an empty model\n",
    "# Sequential means we'll make a group\n",
    "# of a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(5, input_shape=(len(word_index.keys()),)))\n",
    "\n",
    "# output Layer\n",
    "model.add(Dense(len(word_index.keys()), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'SGD',\n",
    "                 loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5627\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5214\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 943us/step - loss: 2.4822\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 976us/step - loss: 2.4450\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 941us/step - loss: 2.4094\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3753\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.3424\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3106\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.2797\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.2499\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.2208\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.1924\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.1647\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.1376\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 924us/step - loss: 2.1112\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.0853\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.0599\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 962us/step - loss: 2.0351\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 968us/step - loss: 2.0109\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 914us/step - loss: 1.9870\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 866us/step - loss: 1.9637\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 984us/step - loss: 1.9409\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 978us/step - loss: 1.9187\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 997us/step - loss: 1.8969\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8757\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8550\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8348\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8152\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7962\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7776\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7597\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7424\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7257\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7095\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6937\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6787\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6641\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6500\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6364\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6235\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6109\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5987\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5869\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5754\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 981us/step - loss: 1.5646\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 913us/step - loss: 1.5540\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 889us/step - loss: 1.5435\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 870us/step - loss: 1.5336\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 872us/step - loss: 1.5239\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 943us/step - loss: 1.5145\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 998us/step - loss: 1.5053\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4963\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4874\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4792\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4705\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4627\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4547\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4468\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4392\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4317\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4243\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4171\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4094\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4028\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3958\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 961us/step - loss: 1.3882\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 957us/step - loss: 1.3821\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 925us/step - loss: 1.3750\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 917us/step - loss: 1.3684\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 881us/step - loss: 1.3618\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 963us/step - loss: 1.3549\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 998us/step - loss: 1.3488\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3424\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3356\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3296\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 981us/step - loss: 1.3232\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3169\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3101\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3041\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2977\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2913\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2849\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2789\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 947us/step - loss: 1.2722\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 952us/step - loss: 1.2658\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 949us/step - loss: 1.2599\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2534\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2473\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2410\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2349\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2284\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2221\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2158\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2095\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2035\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1970\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 935us/step - loss: 1.1907\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 904us/step - loss: 1.1849\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1788\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1722\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1663\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1603\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 944us/step - loss: 1.1542\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 967us/step - loss: 1.1478\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 982us/step - loss: 1.1422\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 912us/step - loss: 1.1363\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 845us/step - loss: 1.1306\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 860us/step - loss: 1.1247\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 937us/step - loss: 1.1190\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1130\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 909us/step - loss: 1.1066\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 975us/step - loss: 1.1015\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0964\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 969us/step - loss: 1.0906\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0852\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 971us/step - loss: 1.0800\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 962us/step - loss: 1.0745\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0693\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 990us/step - loss: 1.0643\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 963us/step - loss: 1.0591\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0541\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 971us/step - loss: 1.0490\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0434\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0393\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0340\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0291\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0254\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0209\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0158\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0119\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0077\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0036\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 971us/step - loss: 0.9993\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 956us/step - loss: 0.9949\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 868us/step - loss: 0.9900\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 853us/step - loss: 0.9870\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 983us/step - loss: 0.9835\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9798\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 857us/step - loss: 0.9756\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.9720\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.9688\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9648\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9609\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9584\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9547\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 981us/step - loss: 0.9517\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 909us/step - loss: 0.9483\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 891us/step - loss: 0.9444\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.9424\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 868us/step - loss: 0.9388\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 963us/step - loss: 0.9357\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 844us/step - loss: 0.9327\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 882us/step - loss: 0.9309\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 904us/step - loss: 0.9278\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 852us/step - loss: 0.9238\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 959us/step - loss: 0.9220\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9194\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9171\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9137\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 958us/step - loss: 0.9114\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 961us/step - loss: 0.9091\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9061\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9040\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9021\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8984\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8974\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8948\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8925\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8908\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8873\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8863\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8842\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8825\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8799\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8783\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8765\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8743\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8722\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8706\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8678\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8666\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8650\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8632\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8620\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8604\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8583\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8564\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8555\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8537\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8521\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8508\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8493\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8478\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8464\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8448\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8429\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8420\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8403\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8394\n",
      "Epoch 200/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8377\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8361\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8354\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8337\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8322\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8312\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8306\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8290\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8280\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8260\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8247\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8244\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8236\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8201\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8193\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8182\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8174\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8181\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8171\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8157\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8151\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8140\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8125\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8113\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8111\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8091\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8086\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8077\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8075\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8060\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8034\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8022\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8040\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8032\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8025\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8002\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7999\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8001\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7984\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7975\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7969\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7960\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7965\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7957\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7948\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7936\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7924\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7928\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7924\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7905\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7911\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7887\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7890\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7894\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7878\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7862\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7865\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7864\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7859\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7854\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7847\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7839\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7840\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7831\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7827\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7820\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7819\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7811\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7808\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7800\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7796\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7792\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7785\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7776\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7784\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7773\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7743\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7759\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7766\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7742\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7740\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7744\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7735\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7745\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7734\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7714\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7737\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7722\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7718\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7695\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7720\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7717\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7702\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7706\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7697\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7699\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7694\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7684\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7661\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7682\n",
      "Epoch 300/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7673\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7664\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7680\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7672\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7663\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7669\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7665\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7656\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7661\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7653\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7651\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7650\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7620\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7642\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 972us/step - loss: 0.7642\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 861us/step - loss: 0.7639\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 844us/step - loss: 0.7631\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 892us/step - loss: 0.7639\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 861us/step - loss: 0.7617\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 859us/step - loss: 0.7631\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 887us/step - loss: 0.7631\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.7627\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 839us/step - loss: 0.7617\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.7620\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 816us/step - loss: 0.7604\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.7619\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 819us/step - loss: 0.7613\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 836us/step - loss: 0.7607\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 888us/step - loss: 0.7603\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7604\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 872us/step - loss: 0.7597\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 887us/step - loss: 0.7608\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.7605\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.7583\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 914us/step - loss: 0.7598\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 896us/step - loss: 0.7590\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 906us/step - loss: 0.7599\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 868us/step - loss: 0.7582\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 901us/step - loss: 0.7588\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 861us/step - loss: 0.7593\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 979us/step - loss: 0.7581\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 951us/step - loss: 0.7582\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 870us/step - loss: 0.7582\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7582\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 878us/step - loss: 0.7552\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 847us/step - loss: 0.7569\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 840us/step - loss: 0.7571\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 858us/step - loss: 0.7574\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 848us/step - loss: 0.7575\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.7573\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 838us/step - loss: 0.7555\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 821us/step - loss: 0.7564\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 829us/step - loss: 0.7542\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 887us/step - loss: 0.7569\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 862us/step - loss: 0.7566\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.7561\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 824us/step - loss: 0.7564\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 828us/step - loss: 0.7551\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 851us/step - loss: 0.7560\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 954us/step - loss: 0.7551\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.7556\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 890us/step - loss: 0.7552\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 902us/step - loss: 0.7554\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 874us/step - loss: 0.7541\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 924us/step - loss: 0.7547\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 859us/step - loss: 0.7551\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.7535\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 855us/step - loss: 0.7551\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 861us/step - loss: 0.7547\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 874us/step - loss: 0.7543\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 802us/step - loss: 0.7542\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 887us/step - loss: 0.7542\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 810us/step - loss: 0.7542\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7520\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7541\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 989us/step - loss: 0.7534\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7540\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7535\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7535\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7512\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7529\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7533\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 929us/step - loss: 0.7527\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7519\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 862us/step - loss: 0.7527\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 904us/step - loss: 0.7520\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7522\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 870us/step - loss: 0.7511\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 882us/step - loss: 0.7534\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.7529\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 996us/step - loss: 0.7522\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.7515\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7494\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 913us/step - loss: 0.7505\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7513\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 917us/step - loss: 0.7508\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.7510\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7520\n",
      "Epoch 398/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 853us/step - loss: 0.7508\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 900us/step - loss: 0.7507\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7509\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 869us/step - loss: 0.7519\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7504\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7516\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7515\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7512\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 949us/step - loss: 0.7494\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7507\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7500\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7509\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7468\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 949us/step - loss: 0.7508\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7507\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 941us/step - loss: 0.7507\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7475\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7480\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7509\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7476\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7494\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 894us/step - loss: 0.7492\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7489\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 897us/step - loss: 0.7503\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7496\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7489\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7494\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7479\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7504\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7485\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7492\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7457\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7500\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7473\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7472\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7487\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7500\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7494\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7493\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7470\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 966us/step - loss: 0.7460\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.7493\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 886us/step - loss: 0.7482\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7466\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.7473\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7476\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 953us/step - loss: 0.7478\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 989us/step - loss: 0.7461\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7491\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 983us/step - loss: 0.7474\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7488\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7473\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7462\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7483\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.7465\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7475\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 964us/step - loss: 0.7475\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.7484\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 895us/step - loss: 0.7467\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 890us/step - loss: 0.7468\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 885us/step - loss: 0.7463\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7465\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7477\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7476\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7472\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7476\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7435\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7461\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7464\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7467\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7461\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7467\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7480\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7475\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7454\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7471\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7454\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7468\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7461\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7466\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7469\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7467\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7459\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7468\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 946us/step - loss: 0.7465\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 922us/step - loss: 0.7446\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 900us/step - loss: 0.7451\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 923us/step - loss: 0.7454\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 888us/step - loss: 0.7461\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 934us/step - loss: 0.7460\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 850us/step - loss: 0.7465\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 907us/step - loss: 0.7451\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7467\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 919us/step - loss: 0.7455\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7466\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7456\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 976us/step - loss: 0.7437\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7459\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7450\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7437\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 972us/step - loss: 0.7465\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 901us/step - loss: 0.7430\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c99f2a940>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X.transpose(),\n",
    "           y.transpose(),\n",
    "           epochs = 500,\n",
    "           batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Word Embedding\n",
    "Now we need to get the weight matrix. We didn't review how to do this in the `Neural Networks` folder so let's see how to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights.append(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[-0.49569863,  0.7690507 ,  0.2582959 ,  1.117975  , -0.7778494 ],\n",
       "         [ 0.5211052 , -1.3855205 , -0.29717094, -0.9724434 ,  0.6368756 ],\n",
       "         [-1.0505626 ,  0.08296789, -1.1196201 ,  0.02472505,  0.27013054],\n",
       "         [-0.8911259 ,  0.18795282, -1.011026  , -0.56631684, -0.40701547],\n",
       "         [ 0.8934771 ,  0.09746572,  1.2973902 , -0.14058812, -0.18869022],\n",
       "         [ 0.4917729 ,  0.46411788, -0.32895672,  0.08060575,  0.52682984],\n",
       "         [-0.43770885, -0.44972926,  0.46853316, -0.48093465,  0.08606547],\n",
       "         [ 0.6577237 ,  0.0275485 ,  1.4407226 ,  0.20522985,  0.42937624],\n",
       "         [ 0.17962795, -0.22618899, -0.16606489, -0.38668835, -0.41202536],\n",
       "         [-0.8152782 , -0.05408753, -1.1971552 ,  0.25684068,  0.12354874],\n",
       "         [ 0.810353  ,  0.13236481,  1.2305428 ,  0.4473609 ,  0.60293067],\n",
       "         [-0.28364512,  0.5903325 ,  0.09090793,  0.04105729,  0.52238655]],\n",
       "        dtype=float32),\n",
       "  array([-0.89970356, -0.87718844,  1.3947489 , -0.6022218 ,  0.42835045],\n",
       "        dtype=float32)],\n",
       " [array([[ 0.20499645, -0.33827996,  0.43138912, -0.37802425,  0.32488206,\n",
       "           0.27264437, -1.7950677 ,  0.1978831 ,  1.1047218 , -0.37324312,\n",
       "          -0.2947729 , -1.2344469 ],\n",
       "         [ 0.38256806, -0.01837515, -0.10583869,  0.52175176,  0.11949237,\n",
       "          -0.05635729,  0.10934338,  0.48235053, -0.87910146,  0.4185225 ,\n",
       "           0.3130099 , -1.2654226 ],\n",
       "         [-0.38434556, -0.01411935, -0.574436  , -0.35674176, -0.16026743,\n",
       "           2.5602756 ,  0.08924817,  0.0136006 ,  1.9744016 , -0.550301  ,\n",
       "          -0.47720328, -0.53249335],\n",
       "         [ 0.452404  ,  0.07655676,  0.33091727, -0.28929305,  0.38360444,\n",
       "           0.68009347, -0.02924616,  0.22930579, -0.9283155 ,  0.11044426,\n",
       "          -0.32782376, -1.0985363 ],\n",
       "         [-0.6004694 , -0.39245978,  0.16442709,  0.00312016, -0.29510647,\n",
       "          -0.39924103, -0.31303918, -0.25028786,  0.8621106 , -0.4989472 ,\n",
       "          -0.5434678 ,  0.42284283]], dtype=float32),\n",
       "  array([-0.47208953, -0.87477136, -0.58117115, -0.701452  , -0.61087805,\n",
       "          0.8125744 ,  2.453465  , -0.6419775 ,  0.31129363, -0.53431845,\n",
       "         -0.62319255,  1.4625002 ], dtype=float32)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the 0th entry of the 0th entry in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weights[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting to 2-dimensions\n",
    "Now we can look at the word embedding in two dimensions using a standard dimension reduction technique like PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit=pca.fit_transform(weights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(fit[:,0],fit[:,1])\n",
    "\n",
    "for i in range(len(word_index.keys())):\n",
    "    plt.text(fit[i,0],fit[i,1],reverse_index[i], fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with this notebook, I want to pause and show you a nice interactive web app that gives a good intuition for Word2Vec, https://ronxin.github.io/wevi/. Go to that app and play around for a bit before finishing this notebook.\n",
    "\n",
    "## Training Word2Vec is Costly\n",
    "Before moving on to the next notebook let's end with this final demonstration.\n",
    "\n",
    "Program a loop to count the number of skip-grams that would result from the imdb data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(imdb_train)):\n",
    "    for gram in sequence.skipgrams(imdb_train[i], \n",
    "                                  vocabulary_size=n,\n",
    "                                  window_size=window_size,\n",
    "                                  negative_samples=0)[0]:\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The imdb training data set would produce 23721364 skip-grams\n"
     ]
    }
   ],
   "source": [
    "print(\"The imdb training data set would produce\", count, \"skip-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That's a lot of data!\n",
    "\n",
    "To make things worse (from your laptop's perspective) in the imdb example $M =10,000$ and a standard $N$ is $300$  (based on original paper). That means we'd need to find weights for $10,000\\times 300 = 3,000,000$  weights twice. (Good thing we have a lot of data).\n",
    "\n",
    "So training your own Word2Vec embedding comes with a large start up cost compared to everything else we've done in the program.\n",
    "\n",
    "That's why many projects don't start with the training of a custom Word2Vec embedding, but first either try some of the older techniques we've learned or use a pretrained Word2Vec embedding, the topic of Next Week's Class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
